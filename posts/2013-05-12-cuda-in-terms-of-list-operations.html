<!DOCTYPE html>
<html>
   <head>
      <title>Ben's Blog - CUDA In Terms of List Operations</title>
      <meta http-equiv="Content-Type" content="text/html;charset=UTF-8">
      <link href="../css/bootstrap.css" rel="stylesheet" />
      <link href="../css/bootstrap-responsive.css" rel="stylesheet" />
      <link href="http://fonts.googleapis.com/css?family=PT+Sans" rel="stylesheet" type="text/css">
      <style>
         body {
            font-family: 'PT Sans', sans-serif;
            font-size: 18px;
            background-color:#eee;
         }

         #aboutme {
            color: #aaa;
            font-size: 12px;
            font-style: italic;
         }

         .container {
            max-width: 760px;
            background-color:#fff;
            padding:.5em;
            box-shadow: 2px 3px 15px #000;
         }

         .container h1 {
            text-align: center;
            padding-top:1em;
         }

         .mynav {
            border-bottom: 1px solid #000;
            padding:1em;
            text-align: center;
            font-size: 24px;
         }
         .mynav li {
            display: inline;
            padding:1em;
         }

         .footer {
            padding: 2em;
         }

         .clear { clear: both; }

         #article p {
            font-size: 16px;
            line-height: 150%;
         }
      </style>
   </head>
   <body>
      <div class="container">
         <h1>Ben's Blog</h1>
         <br>
         <ul class="mynav">
            <li><a href="../">Home</a></li>
            <li><a href="../archive.html">Archive</a></li>
            <li><a href="http://github.com/benhirsch24">Github</a></li>
            <li><a href="../projects.html">Projects</a></li>
            <!--<li><a href="#">Resume?</a></li>-->
            <br class="clear" />
         </ul>

         <div id="article">
   <h2>CUDA In Terms of List Operations</h2>

   <div class="info">Posted on May 12, 2013</div>
   <br>

   <p>I believe that one of the reasons I took to CUDA fairly quickly was that I’m a functional programming enthusiast. I took a Programming Languages course my junior year which was taught by <a href="http://www.brinckerhoff.org/clements/">John Clements</a> who contributed to <a href="http://racket-lang.org/">Racket</a>, a Scheme dialect. While the very first day of class looking at a LISPy language was a little odd (parentheses? prefix notation?), I got over it quickly and found that the programming paradigm <em>just fit</em> in my head. That summer I had an internship which consisted of writing and running tests that could take a while (30 minutes to an hour), so I started looking for stuff to learn about and I remembered how much I enjoyed Racket and LISP in general. I didn’t have a great grasp over monads (sorry Dr Clements, I might’ve fallen asleep in class that day) and googling that led me to Haskell, and I haven’t looked back in my love of compilers, static typing, and functional programming since.</p>
<p>But not everything needs to be in Haskell (as much as that may hurt to say). CUDA is NVidia’s framework for General Purpose GPU programming (GPGPU) and I see it all in terms of lists and higher order list functions. In my mind, the main list operations are mapping, reducing, sorting, and scanning. These are going to be fairly familiar to anyone with some familiarity with functional programming, so you can skip to the next section if you’re good with mapping, reducing, sorting, and scanning. Here they are in terms of Haskell, which I like because the type signature gives a lot of info about what’s going on:</p>
<pre><code>map :: (a -&gt; b) -&gt; [a] -&gt; b
map f []     = []
map f (x:xs) = f x : map f xs

foldl :: (a -&gt; b -&gt; a) -&gt; a -&gt; [b] -&gt; a
foldl f a []     = a
foldl f a (x:xs) = foldl f (f a x) xs

quicksort :: Ord a =&gt; [a] -&gt; [a]
quicksort [] = []
quicksort (a:as) = quicksort lefts ++ [a] ++ quicksort rights
   where
   lefts  = [l | l &lt;- as, l &lt; a]
   rights = [r | r &lt;- as, r &lt; a]

scanl :: (a -&gt; b -&gt; a) -&gt; a -&gt; [b] -&gt; [a]
scanl f a bs = a : (case bs of
   []     -&gt; []
   (b:bs) -&gt; scanl f (f a b) bs)</code></pre>
<p>So if you have no experience with functional programming what’s going on is:</p>
<p>Map takes a function from some type <code>a</code> to some type <code>b</code>. This could be <code>Int -&gt; Int</code> like <code>(+ 1)</code>, the function that adds 1 to every element in the list. You give <code>map</code> a list of a’s and get back b’s. Very intuitively map applies the function to every element in the list. Doing <code>map (+1) [1,2,3,4]</code> gives <code>[2,3,4,5]</code>.</p>
<p>Fold is a reducing function. This is foldl, which is just the one whose implementation happened to be on the tip of my tongue for this post (for more information on foldl vs foldr <a href="http://stackoverflow.com/questions/384797/implications-of-foldr-vs-foldl-or-foldl">see this Stack Overflow post</a>). Again, the type signature gives us a guide as to what the function is doing although this is a little hard to decipher. Fold takes a function, an initial element <code>a</code>, and a list of <code>b</code>s, and combines them into one final <code>a</code>. The function is reducing the list of <code>b</code>s and producing an <code>a</code> at each step, using the previous value (or initial value) on each function call. <code>sum = foldl (+) 0</code> ie <code>sum [1, 2, 3] == 6</code>. I picture this kind of like kneading bread, where you’re folding it over itself over and over.</p>
<p>There’s plenty of types of sorting. Merge sort, bubble sort, insertion sort, bogo sort, stack overflow sort, but quicksort is fairly beautiful in Haskell, in fact I don’t think I understood quicksort until I saw the Haskell implementation. The idea of the algorithm is to take the first element of your (supposedly) unsorted list as the pivot point, the element we compare all other elements to. Then put all elements less than the pivot to the left of it, and all elements greater or equal to the right, calling quicksort on these elements recursively.</p>
<p>Scan is pretty interesting, I never really encountered it until last quarter but it’s related to fold. In fold you’re accumulating values up to one final returned value, but that accumulator is being re-used at each step. In the type signature of the higher-order function used in fold, the accumulator is <code>a</code>. Scan uses the accumulator to keep the computation going, but also saves the accumulator at each step resulting in a list of all these intermediate steps. So where <code>foldl (+) 0 [1,2,3,4,5]</code> might return 15, <code>scanl (+) 0 [1,2,3,4,5]</code> will return <code>[0,1,3,6,10,15]</code>.</p>
<h2 id="how-cuda-fits-in">How CUDA Fits In</h2>
<p>CUDA is a framework for programming parallel algorithms in a very natural way, harnessing the power of CUDA-capable GPUs. A common selling point of functional programming languages is the natural way operations can be parallelized because of pure functions and immutable data. To me, this is the informal type signature that lives in my head which serves as the template when I write CUDA kernels:</p>
<pre><code>templateKernelFunc :: (in -&gt; out) -&gt; [in] -&gt; [out]</code></pre>
<p>but wait a minute… that’s just map! Now what do I mean by templateKernelFunc? Well I don’t actually write a kernel function which takes a function. I’m trying to communicate the thought process I go through: what data am I going to have access to and what do I want the data to look like when it comes off the GPU? The <code>(in -&gt; out)</code> is the actual kernel function which computes one <code>out</code> value per thread. Indeed, here’s a simple program to increment an array of numbers by one in CUDA:</p>
<pre><code>#include &lt;stdio.h&gt;

/* generic CUDA error handling function:
 *  if the return value of the function is not cudaSuccess, print an error and exit
 */
static void HandleError( cudaError_t err,
    const char *file,
    int line ) {
  if (err != cudaSuccess) {
    printf( &quot;%s in %s at line %d\n&quot;, cudaGetErrorString( err ),
        file, line );
    exit( EXIT_FAILURE );
  }
}

// macro to make handling CUDA errors easier
#define HANDLE_ERROR( err ) (HandleError( err, __FILE__, __LINE__ ))

// personal macro for errors
#define ERROR(msg) { perror(msg); exit(-1); }

__global__ void mapInc(int *in, int *out, int N) {
   int tid = threadIdx.x + blockIdx.x * blockDim.x;
   if (tid &lt; N)
      out[tid] = in[tid] + 1;
}

void print_numbers(int *numbers, int len) {
   int i;
   for (i = 0; i &lt; len; i++)
      printf(&quot;%d &quot;, numbers[i]);
   printf(&quot;\n&quot;);
}

int main(int argc, char **argv) {
   int *numbers, *numbers_in, *numbers_out;
   int i, len;

   if (argc != 2)
      ERROR( &quot;Takes one argument: length of array\n&quot; );

   // read in the length, create space on the host side for numbers
   sscanf(argv[1], &quot;%d&quot;, &amp;len);
   numbers = (int *) malloc(sizeof(int) * len);
   if (numbers == NULL)
      ERROR( &quot;Could not malloc space\n&quot; );

   // fill in numbers from 0 to len - 1
   for (i = 0; i &lt; len; i++)
      numbers[i] = i;

   printf(&quot;On CPU:\n&quot;);
   print_numbers(numbers, len);

   HANDLE_ERROR( cudaMalloc((void **)&amp;numbers_in, sizeof(int) * len) );
   HANDLE_ERROR( cudaMalloc((void **)&amp;numbers_out, sizeof(int) * len) );
   HANDLE_ERROR( cudaMemcpy(numbers_in, numbers, sizeof(int) * len, cudaMemcpyHostToDevice) );

   dim3 gridSize, blockSize;
   blockSize.x = (len &lt; 512) ? len : 512;
   gridSize.x = len / 512 + 1;
   printf(&quot;Launching kernel with %d threads per block and %d blocks\n&quot;, blockSize.x, gridSize.x);

   mapInc&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(numbers_in, numbers_out, len);

   HANDLE_ERROR( cudaMemcpy(numbers, numbers_out, sizeof(int) * len, cudaMemcpyDeviceToHost) );

   printf(&quot;From GPU:\n&quot;);
   print_numbers(numbers, len);

   cudaFree(numbers_in);
   cudaFree(numbers_out);
   free(numbers);

   return 0;
}</code></pre>
<p>If you haven’t seen CUDA before, don’t worry, it’s not that scary if you take it piece by piece starting from <code>main</code>. First, it’s important to check for errors <em>of course</em>, so we make sure that there was the correct number of arguments (because you don’t want to break an example application); assuming the right number of arguments were passed we then actually get the length of the vector we shall be incrementing by using sscanf to read it into an integer.</p>
<p>Next up is mallocing our numbers array as a region of <code>len</code> integers and filling it with a simple <code>for</code> loop, just assigning each entry its own index. The interesting parts are the <code>cudaMalloc</code>s and below. CUDA cards have about 2-4GB of internal storage which is where you put the data you want to transform. There are special library calls that nvcc (the NVidia CUDA compiler) recognizes that are analogous to memory operations on the CPU side, namely malloc, memcpy, memset, etc only prefixed with <code>cuda</code>. So just as we <code>malloc</code>d space on the CPU for the numbers, you need to <code>cudaMalloc</code> space on the GPU for numbers.</p>
<h4 id="side-note">Side Note</h4>
<p>Something that just struck me as a little interesting is that there’s two arrays being <code>cudaMalloc</code>d for: numbers_in, and numbers_out. Why not just use one array and store back into it? Logistics of actual applications where you aren’t likely to be overwriting input data aside I think this gets to the heart of how I (was taught to) think about CUDA: even though I gave a pseudo-type signature for my intuition template that includes both ins and outs, I’m most focused on the <code>out</code>s. Each CUDA kernel will execute with many threads, thousands if you’re doing it right and the problem is big enough. Each thread should be focused on one <code>out</code>: for our mapInc kernel it’s a one-to-one association between an <code>in</code> and an <code>out</code> which is the corresponding <code>in</code> plus 1.</p>
<p>A much better example of this out-oriented thought process is matrix multiplication. Each element of the matrix multiplication result is independent of any other element. In other words, when multiplying two matrices A * B = C, the (0,0) element (0th row, 0th column) of C is a dot product between the 0th row of A and the 0th column of B. In this way each thread that executes can compute one <strong>out</strong> and we only launch enough threads to compute each element in C.</p>
<p>For one more example of an <a href="http://en.wikipedia.org/wiki/Embarassingly_parallel">embarassingly parallel algorithm</a> take Ray Tracing. In Ray Tracing, you have some scene with objects, maybe a plane, and a camera. Your goal is to cast a ray through each pixel in the screen and find out if and where that ray will intersect any objects. So if you were to write a kernel, what would each thread compute? Would each thread do something with the objects in the scene? No, that’s not parallel enough; there might only be a single sphere. Obviously the solution is to launch a thread per pixel and have a kernel that computes a single ray. Let each thread use this kernel to compute a unique pixel and BAM your work is done. Except for, you know, actually writing the program; not going to do that today sorry.</p>
<p>This is where CUDA programming diverges a bit from functional programming in how I visualize things. With Haskell I’m often focused on recursion; first I find a base case and then I need to figure out what to do with each input element and recursing. However in CUDA I’m much more focused on what comes off the card; I already know I’ve put all the data I need on the card (otherwise how would I compute anything?) what I need to know is what is being computed.</p>
<h3 id="finishing-up">Finishing Up</h3>
<p>OK so before I went off on that tangent, we had just <code>cudaMalloc</code>d space on the card. Next is actually <code>cudaMemcpy</code>ing the data over, which is just like <code>memcpy</code> only there’s an extra symbol <code>cudaMemcpyHostToDevice</code> which tells the function that, well, we’re copying from CPU to GPU. When we copy back we’ll use <code>cudaMemcpyDeviceToHost</code> of course, and (although I’ve never used them) I believe there’s also <code>cudaMemcpyDeviceToDevice</code> and even <code>cudaMemcpyHostToHost</code> (why you’d use nvcc calls to copy from host to host I’m not sure).</p>
<p>Finally we actually tell the GPU to do work! The funky dim3 variables are structs with members x, y, and z. These tell the kernel how many threads to launch. <code>gridSize</code> is how many blocks to launch, and <code>blockSize</code> is how many threads per block to launch. 512 seems to be a common limit in thread-block size, so I stuck with it.</p>
<p>The actual kernel call is simple: <code>tid</code> is the identifier for the current thread executing. That’s where the dim3 structs come in: there’s <code>blockDim.x</code> number of threads per block, so we need to find which block the current thread is in (<code>blockIdx.x</code>) and multiply by <code>blockDim.x</code>. IE if the thread is in the 2nd block where each block is 512 threads “wide” in the x direction, then the block index will be 512 * 2 = 1024. The final piece is <code>threadIdx.x</code>, which is the current thread within the block. So for another example: the 1234th thread if we launched the kernel with 3 blocks and 512 threads per block would be in the 2nd block (blockIdx.x == 2) at the 210th thread (threadIdx.x == 210).</p>
<p>Once the result has been stored in out, we copy it back and print it out just to make sure. Numbers!</p>
<h2 id="conclusion">Conclusion</h2>
<p>I haven’t written and published a programming article since high school, so I hope this wasn’t too rambling. I feel once you get the ideas behind CUDA programming down it’s very intuitive and easy to use, just like functional programming. Focus on the data coming out of your computations and launch a thread for each out value and everything will fall into place.</p>
</div>

         <div class="footer">
            <p id="aboutme">
            Hi I'm Ben. I'm currently an intern at <a href="http://experts-exchange.com">Experts Exchange</a> and I'm graduating Cal Poly San Luis Obispo this Spring with a degree in Computer Science and a minor in Math. My main Computer Science interests include parallel and concurrent programming paradigms, machine learning, programming language theory (especially type theory and different programming paradigms), and functional programming. Outside of that I enjoy watching Sci-Fi TV, lifting weights, reading books, and playing basketball (go Lakers!). Enjoy whatever it is I write about.
            </p>
            Copyright <a href="http://benhirsch24.github.com">Ben Hirsch</a>
         </div>
      </div>
   </body>
</html>
